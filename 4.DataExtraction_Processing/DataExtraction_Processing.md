## 4. Data Extraction and Processing

After creating this application, a time series containing valuable data will be generated. Prometheus is configured to scrape data from the container every 20 seconds and to save it over a 1-day time span. To pull this data, a shell script was created which queries the data acquired in the last 15 minutes, from the container running within the “stress-testing-pod”, using PromQL. It accesses the desired data using Prometheus’ uniform resource locator (URL) combined with the API endpoint utilized by the same Prometheus for this purpose. This data will be extracted inside a .JSON file called “output-metrics.json”, which contains the amount of memory under usage, inside the container and the UNIX timestamp at the extraction time. The entire shell script was added to the appendix of this paper under the name: “Data Fetching Script (vi fetch_memory_usage.sh)”.  After saving the script inside the Ubuntu machine, the command change mode (“chmode”) followed by the flag “+x” are employed.

The dataset extracted contains all the raw inputs scraped throughout the specified extraction interval. The training of the ML model can be optimized by first processing this dataset, to only include the key data for the model training. Seeing as this paper proposes a method for scaling containers in case of memory spikes, which are effectively peaks in memory usage, the extracted data will be subjected to a peak detection script. This python script, attached to the appendix, employs a peak detection function from the signal processing library, ”Scipy”, which will filter the peaks in memory usage. The peak detection function is defined with a minimum peak in mind of 40 MB, to filter out noise data and with a distance of 30 data points between peaks, to ensure that one peak is representing the spike in memory usage, which may run for a longer period. This script can be found saved under the name “Data Processing Python Script” within the appendix. Similarly to the case of the memory leak script, a new namespace was defined utilizing a ConfigMap, to separate the processes running the python scripts. The name of this namespace is “python-scripts”. To run both data manipulation scripts a deployment is defined, tasked with managing one pod running the application described above. The container running the application runs the same lightweight Linux image, Alpine Linux. First, the packages required for running the scripts are installed, after which two CronJobs are set to periodically run the Python scripts at 15 minutes intervals. The first CronJob runs the data acquisition script, while the second CronJob first waits for 15 seconds, to ensure that the data has been written to the JSON file, before initializing the peak detection application. Inside the data processing script, a file is created containing all the dataset queried, having it periodically (the periodicity is set by the CronJob running the command) append more data to itself, as every 15 minutes, 15 minutes’ worth of data is added to that comma separated values (CSV) file. The same is done with the data subjected to peak detection, which is periodically appended to another CSV file, called “all_peak_data.csv”.

To make the python scripts available within the container running the CronJob applications, a volumeMount was created, having the host path defined as the one containing the scripts. Thus, every file contained inside the path will be also available inside the container memory.

```bat
  echo "*/15 * * * * cd /python-scripts && pwd && /bin/sh fetch_memory_usage.sh >> fetch_memory.log 2>&1" > /etc/crontabs/root &&
  echo "*/15 * * * * cd /python-scripts && pwd && sleep 15 && /usr/bin/python3 data_processing.py >> fetch_memory.log 2>&1" >> /etc/crontabs/root &&
```
