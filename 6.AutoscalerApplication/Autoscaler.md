## 6. Autoscaler Application

The final step of the implementation is to enact the actual autoscaler. To do this, the Kubernetes command “patch” was utilized. Using this command the memory limits and requests of memory can be dynamically adjusted. For the purpose of this experiment, to determine the difference the scaler makes whilst running, a policy may be set within the container, namely “resizePolicy”, of the memory in this case, should be set to the value “NotRequried”. This way, changes made to the memory allocation of the container using the patch command will not automatically restart the container.

The final implementation will be done using the forecast generated by the ARIMA model. In the previous steps, it has been determined that the model will save the forecasted memory spike timestamp within a CSV file called: “next_peak_arima.csv”. To do the forecast, a python script has been employed which will analyze the forecasted timestamp and generate two more variables, one containing the scale-up time of the stress-pod and the other, the reset of the scaling to its initial conditions. The scale-up will occur 6 minutes before the memory spike and it will last a total of 12 minutes, meaning 6 minutes after the forecast as well. The two commands used to patch the properties of the container look like this: one is used to scale up the container, by allocating 1 GB of memory and setting the limit up to 1.5 GB, while the other is used to scale the application back down, to its original resource limits, so that resources are freed up when not required. Two while loops compare the scale-up/ scale-down timestamps to the current time, to ensure the modifications happen on time. The modifications made to the container are also saved within a log file.

```bat
  scale_up_command = (
      "kubectl patch deployment stress-testing-deployment "
      "--namespace=stress-test "
      "--patch '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"stress-testing-container\","
      "\"resources\":{\"requests\":{\"memory\":\"1Gi\"}, \"limits\":{\"memory\":\"1.5Gi\"}}}]}}}}'"
  )
  scale_down_command = (
      "kubectl patch deployment stress-testing-deployment "
      "--namespace=stress-test "
      "--patch '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"stress-testing-container\","
      "\"resources\":{\"requests\":{\"memory\":\"512Mi\"}, \"limits\":{\"memory\":\"512Mi\"}}}]}}}}'"
  )

```
Now that the methodology of work has been described, the patching commands need to be run periodically, which is done by utilizing another CronJob. This has been defined within the ARIMA deployment manifest file. The Cronjob is designated to run the python script right after the model’s forecasts, both of which are periodic. 

```bat
echo "20,50 * * * * cd /python-scripts && /usr/local/bin/python arima_model.py >> forecast.log 2>&1" > /etc/cron.d/arima-forecast && 
echo "20,50 * * * * cd /python-scripts && sleep 1m && /usr/local/bin/python scaling.py >> scaling.log 2>&1" >> /etc/cron.d/arima-forecast &&
```
After deploying this manifest file using “microk8s kubectl apply -f”, the stress-pod’s autoscaler will be continuously running and outputting the scaling operations performed inside a log file. In the next section, the practical results of this paper’s implementation will be presented. 

